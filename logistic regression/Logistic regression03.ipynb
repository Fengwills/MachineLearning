{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is the special case of linear regression where dependent or output variable is categorical. Logistic in logistic regression comes from the function which is the core of this algorithm. This function is logistic function or sigmoid function . Logistic function can be written as : \n",
    "\n",
    "$$f(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "![Sigmoid function](images/sigmoid.png)\n",
    "\n",
    "Logistic regression is the classification algorithm.  We can implement this function in numpy as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "#mpl.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-20.        , -19.18367347, -18.36734694, -17.55102041,\n",
       "       -16.73469388, -15.91836735, -15.10204082, -14.28571429,\n",
       "       -13.46938776, -12.65306122, -11.83673469, -11.02040816,\n",
       "       -10.20408163,  -9.3877551 ,  -8.57142857,  -7.75510204,\n",
       "        -6.93877551,  -6.12244898,  -5.30612245,  -4.48979592,\n",
       "        -3.67346939,  -2.85714286,  -2.04081633,  -1.2244898 ,\n",
       "        -0.40816327,   0.40816327,   1.2244898 ,   2.04081633,\n",
       "         2.85714286,   3.67346939,   4.48979592,   5.30612245,\n",
       "         6.12244898,   6.93877551,   7.75510204,   8.57142857,\n",
       "         9.3877551 ,  10.20408163,  11.02040816,  11.83673469,\n",
       "        12.65306122,  13.46938776,  14.28571429,  15.10204082,\n",
       "        15.91836735,  16.73469388,  17.55102041,  18.36734694,\n",
       "        19.18367347,  20.        ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = np.linspace(-20,20,50) #generate a list of numbers\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.06115362e-09, 4.66268920e-09, 1.05478167e-08, 2.38610019e-08,\n",
       "       5.39777490e-08, 1.22107080e-07, 2.76227484e-07, 6.24874560e-07,\n",
       "       1.41357420e-06, 3.19774584e-06, 7.23382998e-06, 1.63640365e-05,\n",
       "       3.70175420e-05, 8.37362281e-05, 1.89405944e-04, 4.28366894e-04,\n",
       "       9.68517025e-04, 2.18827951e-03, 4.93663522e-03, 1.10983776e-02])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will pass each number through sigmoid function\n",
    "results = sigmoid(numbers)\n",
    "results[:20]  #print few numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all numbers are squashed between [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement logistic regression using sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#this time we will use digit dataset.\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data  #input\n",
    "y = digits.target #output\n",
    "print(digits.data.shape)  #1797 samples * 64 (8*8)pixels\n",
    "#input is an image and we would like to train a model which can predict the digit that image contains\n",
    "#each image is of 8 * 8 pixels\n",
    "\n",
    "#plot few digits  ## dont worry if u dont understand it\n",
    "fig = plt.figure()\n",
    "plt.gray()\n",
    "ax1 = fig.add_subplot(231)\n",
    "ax1.imshow(digits.images[0])\n",
    "\n",
    "ax2 = fig.add_subplot(232)\n",
    "ax2.imshow(digits.images[1])\n",
    "\n",
    "ax3 = fig.add_subplot(233)\n",
    "ax3.imshow(digits.images[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we dont need to preprocess our dataset, we will directly move to third step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "#train a model\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Test a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99332220367278801"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sklearn provides several ways to test a classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, log_reg.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99332220367278801"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another way\n",
    "log_reg.score( X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please recall that its not a good thing to test a model on training dataset. As you can see, we are getting almost 100% accuracy and the reason is we are testing a model on the dataset on which we trained it. Its like you got examples in your test paper same as you practiced during the lecture. So, deifnitely you will get full marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0, 179,   0,   1,   0,   0,   0,   0,   2,   0],\n",
       "       [  0,   0, 177,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0, 183,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0, 181,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0, 182,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0, 181,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 179,   0,   0],\n",
       "       [  0,   5,   0,   1,   0,   0,   0,   0, 168,   0],\n",
       "       [  0,   0,   0,   1,   0,   0,   0,   0,   2, 177]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix is a table that can be used to evaluate the performance of a classifier\n",
    "#each row shows actual values and column values shows predicted values\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y, log_reg.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix is a table that can be used to evaluate the performance of a classifier. Each row shows actual values and column values shows predicted values. For example, image with digit 9 comes 180 times but our model predicted 177 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted :  [1]\n",
      "Actual :  1\n"
     ]
    }
   ],
   "source": [
    "#we can use predict method to predict the class\n",
    "print(\"Predicted : \" , log_reg.predict(digits.data[1].reshape(1,-1)))\n",
    "print(\"Actual : \", digits.target[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.75045461e-18   9.99447460e-01   7.00809699e-10   3.72475330e-09\n",
      "    2.15616661e-06   1.35167550e-09   5.71303497e-10   1.95595337e-13\n",
      "    5.50377100e-04   5.64607392e-10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can also predict the probability of each class\n",
    "proba = log_reg.predict_proba(digits.data[1].reshape(1,-1)) # second column has the highest probability\n",
    "print(proba)\n",
    "np.argmax(proba) #please note index starts with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice,we divide our dataset into two parts-training and testing part. Lets implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris #https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "iris = load_iris()\n",
    "iris.data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into two parts\n",
    "from sklearn.model_selection import train_test_split\n",
    "#split dataset into 70-30\n",
    "X_train, X_test, y_train , y_test = train_test_split(iris.data, iris.target, test_size= 0.3, random_state=42)\n",
    "#randomstate - to make sure each time we run this code it gives same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4)\n",
      "(105,)\n",
      "(45, 4)\n",
      "(45,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train on training data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97777777777777775"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test on test data\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory part, I explained what is overfitting and how it might affect our model so badly. Next, we will implement various methods such as regularizarion and cross-validation to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, scikit-learn uses l2 regularization with C=1. Please note, C is the inverse of regularization strength. We can tweak some parameters to play around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97777777777777775"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(penalty=\"l2\", C=1) #default configuration\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test) #note, we got same accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let us use l1 regularization\n",
    "model = LogisticRegression(penalty=\"l1\", C=1)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test) #whoa! we got 100% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91111111111111109"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(penalty=\"l2\", C=0.23)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test) \n",
    "\n",
    "#you have to consider various values for this type of parameters (hyperparameters) to find the best one\n",
    "# we can do this with GridCV, RandomCV- This is beyond the scope of this lab session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we discussed about k-fold cross validation. In which we divide whole dataset into k parts and each time we hold out\n",
    "#one part and train on k-1 parts\n",
    "\n",
    "#we'll use boston housing dataset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=5) #k=10\n",
    "\n",
    "costs = []\n",
    "for train_index,test_index in kfold.split(data.data):\n",
    "    X_train, y_train = data.data[train_index], data.target[train_index]\n",
    "    X_test, y_test = data.data[test_index], data.target[test_index]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    costs.append(mean_squared_error(y_test, model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.222843637138403"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93102983468390121"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10 fold cross-validation\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import cross_val_score\n",
    "digits = load_digits()\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "scores = cross_val_score(model,digits.data, digits.target, cv=10, scoring='accuracy' )\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For classification tasks, it is recommended to use variant of KFold- <b> StratifiedFold</b> which preserves the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "digits = load_digits()\n",
    "skfold = StratifiedKFold(n_splits= 10)\n",
    "costs = []\n",
    "for train_index,test_index in skfold.split(digits.data, digits.target):\n",
    "    X_train, y_train = digits.data[train_index], digits.target[train_index]\n",
    "    X_test, y_test = digits.data[test_index], digits.target[test_index]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    costs.append(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93102983468390121"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
