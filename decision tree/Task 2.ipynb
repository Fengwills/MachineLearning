{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Implement decison tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: LendingClub Safe Loan  \n",
    "Implement information gain  \n",
    "Implement information gain ratio (optional)  \n",
    "\n",
    "Using 10-fold cross validation for evaluation  \n",
    "Evaluation matrices: Accuracy, Precision, Recall, F1  \n",
    "Choosing deferent maximum depth and check the performance  \n",
    "Plotting figure to demonstrate deferent accuracy in terms of different maximum depth (From 1 to 10)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "loans = pd.read_csv('data/lendingclub/lending-club-data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using safe_loan for target, and setting positive sample to 1, negative sample to -1, reomve bad_loan\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "del loans['bad_loans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using four features \"grade, term, home_ownership, emp_length\"\n",
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the dataset\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "loans = shuffle(loans, random_state = 34)\n",
    "\n",
    "split_line = int(len(loans) * 0.6)\n",
    "train_data = loans.iloc[: split_line]\n",
    "test_data = loans.iloc[split_line:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the features are descrate, using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using pd.get_dummies to obtain one-hot encoding\n",
    "def one_hot_encoding(data, features_categorical):\n",
    "    '''\n",
    "    Parameter\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "    \n",
    "    features_categorical: list(str)\n",
    "    '''\n",
    "    \n",
    "    for cat in features_categorical:\n",
    "        \n",
    "        one_encoding = pd.get_dummies(data[cat], prefix = cat)\n",
    "        \n",
    "        data = pd.concat([data, one_encoding],axis=1)\n",
    "        \n",
    "        del data[cat]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = one_hot_encoding(train_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain feature name\n",
    "one_hot_features = train_data.columns.tolist()\n",
    "one_hot_features.remove(target)\n",
    "one_hot_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain one hot encoding for test data\n",
    "test_data_tmp = one_hot_encoding(test_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame(columns = train_data.columns)\n",
    "for feature in train_data.columns:\n",
    "    if feature in test_data_tmp.columns:\n",
    "        test_data[feature] = test_data_tmp[feature].copy()\n",
    "    else:\n",
    "        test_data[feature] = np.zeros(test_data_tmp.shape[0], dtype = 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set contains 37224 samples and testing set has 9284 samples  \n",
    "All feature is one-hot encoding  \n",
    "targer lable is 1 or -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Implement Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Entropy：\n",
    "$$\n",
    "\\mathrm{Ent}(D) = - \\sum^{\\vert \\mathcal{Y} \\vert}_{k = 1} p_k \\mathrm{log}_2 p_k\n",
    "$$\n",
    "\n",
    "Information Gain：\n",
    "$$\n",
    "\\mathrm{Gain}(D, a) = \\mathrm{Ent}(D) - \\sum^{V}_{v=1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\mathrm{Ent}(D^v)\n",
    "$$\n",
    "\n",
    "if $p = 0$，$p \\log_2p = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_entropy(labels_in_node):\n",
    "    '''\n",
    "    calculating information entropy for node\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    labels_in_node: np.ndarray, for example [-1, 1, -1, 1, 1]\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    float: information entropy\n",
    "    '''\n",
    "    # number of samples\n",
    "    num_of_samples = labels_in_node.shape[0]\n",
    "    \n",
    "    if num_of_samples == 0:\n",
    "        return 0\n",
    "    \n",
    "    # number of positive samples\n",
    "    num_of_positive = len(labels_in_node[labels_in_node == 1])\n",
    "    \n",
    "    # number of negative samples\n",
    "    num_of_negative =                                                                     # YOUR CODE HERE\n",
    "    \n",
    "    # the probability of positive sample\n",
    "    prob_positive = num_of_positive / num_of_samples\n",
    "    \n",
    "    # the probability of negative sample\n",
    "    prob_negative =                                                                       # YOUR CODE HERE\n",
    "    \n",
    "    if prob_positive == 0:\n",
    "        positive_part = 0\n",
    "    else:\n",
    "        positive_part = prob_positive * np.log2(prob_positive)\n",
    "    \n",
    "    if prob_negative == 0:\n",
    "        negative_part = 0\n",
    "    else:\n",
    "        negative_part = prob_negative * np.log2(prob_negative)\n",
    "    \n",
    "    return - ( positive_part + negative_part )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case 1\n",
    "example_labels = np.array([-1, -1, 1, 1, 1])\n",
    "print(information_entropy(example_labels)) # 0.97095\n",
    "\n",
    "# test case2\n",
    "example_labels = np.array([-1, -1, 1, 1, 1, 1, 1])\n",
    "print(information_entropy(example_labels)) # 0.86312\n",
    "    \n",
    "# test case3\n",
    "example_labels = np.array([-1, -1, -1, -1, -1, 1, 1])\n",
    "print(information_entropy(example_labels)) # 0.86312\n",
    "\n",
    "# test case4\n",
    "example_labels = np.array([-1] * 9 + [1] * 8)\n",
    "print(information_entropy(example_labels)) # 0.99750\n",
    "\n",
    "# test case5\n",
    "example_labels = np.array([1] * 8)\n",
    "print(information_entropy(example_labels)) # 0\n",
    "\n",
    "# test case6\n",
    "example_labels = np.array([])\n",
    "print(information_entropy(example_labels)) # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute information gains\n",
    "def compute_information_gains(data, features, target, annotate = False):\n",
    "    ''' \n",
    "    Parameter\n",
    "    ----------\n",
    "        data: pd.DataFrame，sample\n",
    "        \n",
    "        features: list(str)，feature name\n",
    "        \n",
    "        target: str, lable name\n",
    "        \n",
    "        annotate, boolean，print information gains for all features? False by default\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "        information_gains: dict, key: str, feature name\n",
    "                                 value: float，information gains\n",
    "    '''\n",
    "    \n",
    "\n",
    "    information_gains = dict()\n",
    "    \n",
    "    # Fore each feature, compute information gains\n",
    "    for feature in features:\n",
    "        \n",
    "\n",
    "        left_split_target = data[data[feature] == 0][target]\n",
    "        \n",
    "        right_split_target =  data[data[feature] == 1][target]\n",
    "            \n",
    "        # compute inforamtion entropy for left subtree\n",
    "        left_entropy = information_entropy(left_split_target)\n",
    "        \n",
    "        # compute weight for left subtree\n",
    "        left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target))\n",
    "\n",
    "       # compute inforamtion entropy for right subtree\n",
    "        right_entropy =                                                                 # YOUR CODE HERE\n",
    "        \n",
    "        # compute weight for right subtree\n",
    "        right_weight =                                                                  # YOUR CODE HERE\n",
    "        \n",
    "        # compute informatino entropy for data\n",
    "        current_entropy = information_entropy(data[target])\n",
    "            \n",
    "        # compute information gains based on feature\n",
    "        gain =                                                                             # YOUR CODE HERE\n",
    "        \n",
    "        # store information gains\n",
    "        information_gains[feature] = gain\n",
    "        \n",
    "        if annotate:\n",
    "            print(\" \", feature, gain)\n",
    "            \n",
    "    return information_gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case 1\n",
    "print(compute_information_gains(train_data, one_hot_features, target)['grade_A']) # 0.01759\n",
    "\n",
    "# test case 2\n",
    "print(compute_information_gains(train_data, one_hot_features, target)['term_ 60 months']) # 0.01429\n",
    "\n",
    "# test case 3\n",
    "print(compute_information_gains(train_data, one_hot_features, target)['grade_B']) # 0.00370"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Information Gain Ratio\n",
    "Information gain ratio：\n",
    "\n",
    "$$\n",
    "\\mathrm{Gain\\_ratio}(D, a) = \\frac{\\mathrm{Gain}(D, a)}{\\mathrm{IV}(a)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathrm{IV}(a) = - \\sum^V_{v=1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\log_2 \\frac{\\vert D^v \\vert}{\\vert D \\vert}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Gini\n",
    "Gini：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{Gini}(D) & = \\sum^{\\vert \\mathcal{Y} \\vert}_{k=1} \\sum_{k' \\neq k} p_k p_{k'}\\\\\n",
    "& = 1 - \\sum^{\\vert \\mathcal{Y} \\vert}_{k=1} p^2_k.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Gini index for feature $a$：\n",
    "\n",
    "$$\n",
    "\\mathrm{Gini\\_index}(D, a) = \\sum^V_{v = 1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\mathrm{Gini}(D^v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chose the best feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target, criterion = 'information_gain', annotate = False):\n",
    "    '''    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame, \n",
    "    \n",
    "    features: list(str)，\n",
    "    \n",
    "    target: str， \n",
    "    \n",
    "    criterion: str, \n",
    "    \n",
    "    annotate: boolean, default False，\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    best_feature: str, \n",
    "    \n",
    "    '''\n",
    "    if criterion == 'information_gain':\n",
    "        if annotate:\n",
    "            print('using information gain')\n",
    "        \n",
    "        # obtain the information gains\n",
    "        information_gains = compute_information_gains(data, features, target, annotate)\n",
    "    \n",
    "        # Select the best feature according to the maximum information gains\n",
    "        best_feature =                                                                      # YOUR CODE HERE\n",
    "        \n",
    "        return best_feature\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"criterion is abnormal!\", criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Obtain the number of sample for minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain the number of sample for minority class\n",
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    '''\n",
    "    return the number of sample for minority class，For example [1, 1, -1, -1, 1]，return 2\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    labels_in_node: np.ndarray, pd.Series\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    int：\n",
    "    \n",
    "    '''\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    num_of_one =                                                                          # YOUR CODE HERE\n",
    "    \n",
    "    num_of_minus_one =                                                                # YOUR CODE HERE\n",
    "    \n",
    "    return num_of_one if num_of_minus_one > num_of_one else num_of_minus_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case 1\n",
    "print(intermediate_node_num_mistakes(np.array([1, 1, -1, -1, -1]))) # 2\n",
    "\n",
    "# test case 2\n",
    "print(intermediate_node_num_mistakes(np.array([]))) # 0\n",
    "\n",
    "# test3\n",
    "print(intermediate_node_num_mistakes(np.array([1]))) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf(target_values):\n",
    "    '''\n",
    "    Compute the target value of corresponding leaf node, and store node information into dict\n",
    "   \n",
    "    \n",
    "    Parameter:\n",
    "    ----------\n",
    "    target_values: pd.Series, \n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    leaf: dict，leaf node，\n",
    "            leaf['splitting_features'], None，No need to split feature\n",
    "            leaf['left'], None，leaf node do not have left subtree \n",
    "            leaf['right'], None，leaf node do not have right subtree\n",
    "            leaf['is_leaf'], True, is leaf node?\n",
    "            leaf['prediction'], int, the predicton of leaf node\n",
    "    '''\n",
    "    # create leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True}\n",
    "   \n",
    "    # obtain the numbe of positive samples and negative samples\n",
    "    num_ones = len(target_values[target_values == +1])\n",
    "    num_minus_ones = len(target_values[target_values == -1])    \n",
    "\n",
    "    if num_ones > num_minus_ones:\n",
    "        leaf['prediction'] = 1\n",
    "    else:\n",
    "        leaf['prediction'] = -1\n",
    "\n",
    "    # 返回叶子结点\n",
    "    return leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Building decision tree\n",
    " \n",
    "Stop conditions：\n",
    "1. all of the samples have the same lable \n",
    "2. all features have been used \n",
    "3. the depth of node is maximum depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_create(data, features, target, criterion = 'information_gain', current_depth = 0, max_depth = 10, annotate = False):\n",
    "    '''\n",
    "    Parameter:\n",
    "    ----------\n",
    "    data: pd.DataFrame, \n",
    "\n",
    "    features: iterable, \n",
    "\n",
    "    target: str, \n",
    "\n",
    "    criterion: 'str', 'information_gain'\n",
    "\n",
    "    current_depth: int, \n",
    "\n",
    "    max_depth: int, \n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    dict, dict['is_leaf']          : False,\n",
    "          dict['prediction']       : None,\n",
    "          dict['splitting_feature']: splitting_feature, \n",
    "          dict['left']             : dict\n",
    "          dict['right']            : dict\n",
    "    '''\n",
    "    \n",
    "    if criterion not in ['information_gain', 'gain_ratio', 'gini']:\n",
    "        raise Exception(\"criterion is abnormal!\", criterion)\n",
    "    \n",
    "    remaining_features = features[:]\n",
    "    \n",
    "    target_values = data[target]\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "\n",
    "    # Stop condition 1\n",
    "    # all of the samples have the same lable\n",
    "    # Using function intermediate_node_num_mistakes\n",
    "    if                                                                                  # YOUR CODE HERE\n",
    "        print(\"Stopping condition 1 reached.\")\n",
    "        return create_leaf(target_values) \n",
    "    \n",
    "    \n",
    "    # Stop condition 2\n",
    "    # all features have been used，remaining_features is null \n",
    "    if                                                                                  # YOUR CODE HERE\n",
    "        print(\"Stopping condition 2 reached.\")\n",
    "        return create_leaf(target_values)  \n",
    "    \n",
    "    # Stop condition 3\n",
    "    # the depth of node is maximum depth\n",
    "    \n",
    "    if                                                                                  # YOUR CODE HERE\n",
    "        print(\"Reached maximum depth. Stopping for now.\")\n",
    "        return create_leaf(target_values)   \n",
    "\n",
    "    # obtain the optimal feature for splitting\n",
    "    # Using function best_splitting_feature \n",
    "    \n",
    "    splitting_feature =                                                                 # YOUR CODE HERE\n",
    "    \n",
    "    # split data\n",
    "    # left subtree\n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    \n",
    "    # right subtree\n",
    "    right_split =                                                                       # YOUR CODE HERE\n",
    "    \n",
    "    # remove optimal feature from ramaining features\n",
    "    remaining_features.remove(splitting_feature)\n",
    "    \n",
    "    # print splitting feature and the number of samples of left tree and right tree \n",
    "    print(\"Split on feature %s. (%s, %s)\" % (\\\n",
    "                      splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # If all of the samples are splited into one subtree, create left node \n",
    "    # left tree\n",
    "    if len(left_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target])\n",
    "    \n",
    "    # right tree\n",
    "    if len(right_split) == len(data):\n",
    "        print(\"Creating right node.\")\n",
    "        return                                                                          # YOUR CODE HERE\n",
    "\n",
    "    # create left tree recursively \n",
    "    left_tree = decision_tree_create(left_split, remaining_features, target, criterion, current_depth + 1, max_depth, annotate)\n",
    "    \n",
    "    # create right tree recursively \n",
    "    \n",
    "    right_tree =                                                                        # YOUR CODE HERE\n",
    "\n",
    "    # return nodes except left nodes\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a decision tree\n",
    "my_decision_tree = decision_tree_create(train_data, one_hot_features, target, 'gini', max_depth = 6, annotate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dict\n",
    "    \n",
    "    x: pd.Series，sample to be predicted\n",
    "    \n",
    "    annotate： boolean, \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    return the predicted lable\n",
    "    '''\n",
    "    if tree['is_leaf']:\n",
    "        if annotate:\n",
    "            print (\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction']\n",
    "    else:\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate:\n",
    "             print (\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_data.iloc[0]\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True class: %s ' % (test_sample['safe_loans']))\n",
    "print('Predicted class: %s ' % classify(my_decision_tree, test_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the process of decision tree\n",
    "classify(my_decision_tree, test_sample, annotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate the model in testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, data):\n",
    "    '''\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    tree: dict, decision tree model\n",
    "    \n",
    "    data: pd.DataFrame, data\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    predictions：np.ndarray, the predicted lables\n",
    "    '''\n",
    "    predictions = np.zeros(len(data)) \n",
    "    \n",
    "    predictions = classify(tree,)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate decision tree using information gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the maximum depth is 6\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the maximum depth is 6\n",
    "\n",
    "###### Input your results\n",
    "\n",
    "|Accuracy|Precision|Recall|F1\n",
    "-|-|-|-|-\n",
    "Information Gain|0.0|0.0|0.0|0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
